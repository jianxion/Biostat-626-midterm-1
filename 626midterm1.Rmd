---
title: "626midterm1"
output: html_document
date: "2023-03-21"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(MASS)
library(e1071)
library(caret)
library(Hmisc)
library(xgboost)
library(lightgbm)
library(randomForest)
library(nnet)
library(neuralnet)
setwd("/Users/jianxiongshen/Downloads")
set.seed(2023)
training_data <- read.table("training_data.txt", header = TRUE)
testing_data <- read.table("test_data.txt", header = TRUE)
training_data$activity <- ifelse(training_data$activity %in% c("4", "5", "6"), 0, 1)
# submission using xgb  89 accuracy
params <- list(
  objective = "binary:logistic",
  eval_metric = "auc",
  max_depth = 6,
  eta = 0.1,
  subsample = 0.8,
  colsample_bytree = 0.8
)

dtrain <- xgb.DMatrix(as.matrix(training_data[, -1]), label = training_data$activity)
#dtest <- xgb.DMatrix(as.matrix(test[, -1]), label = test$target_variable)
dtest <- xgb.DMatrix(as.matrix(testing_data[, -1]))

model <- xgb.train(params, dtrain, nrounds = 100) 

# Get predictions on the test set
colnames(dtest) <- NULL
pred <- predict(model, dtest)

# Convert the predictions to binary values
pred_binary <- ifelse(pred > 0.005, 1, 0)
table(pred_binary)

```


```{r}
## submission use neuralnet
set.seed(2023)
nn_fit = neuralnet(activity ~ ., data = training_data[,-1], hidden = c(10,5), act.fct = "logistic", linear.output=F)
plot(nn_fit, rep = "best")
#nn_test = compute(nn_fit, testing_data)
nntest2 = predict(nn_fit, testing_data)
predictions = ifelse(nntest2 < 0.5, 0, 1)
table(predictions)

```


```{r}
## using my own testing data to get accuracy for binary classifier
library(dplyr)
set.seed(2023)
training_data <- read.table("training_data.txt", header = TRUE)
testing_data <- read.table("test_data.txt", header = TRUE)
training_data$activity <- ifelse(training_data$activity %in% c("4", "5", "6"), 0, 1)
standardized.X <- as.data.frame(scale(training_data[,c(-1,-2)]))
test <- 1:2267
train.X = standardized.X[-test,]
test.X = standardized.X[test,]
train.Y = training_data$activity[-test]
test.Y = training_data$activity[test]

pca <- prcomp(train.X, center = TRUE, scale. = TRUE)
num_components <- 20

train_data_pca <- predict(pca, train.X)[,1:num_components]
test_data_pca <- predict(pca, test.X)[,1:num_components]

cv <- trainControl(method = "cv", number = 5)

library(e1071)
svm_model <- svm(train.Y ~ ., data = train_data_pca, 
           kernel = "linear", cost = 10, gamma = 0.01, trControl = cv)

my_predictions <- predict(svm_model, newdata = test_data_pca)

my_pred_binary <- ifelse(my_predictions > 0.5, 1, 0)

accuracy <- mean(my_pred_binary == test.Y)
table(my_pred_binary)

accuracy
```



```{r}
training_data2 <- read.table("training_data.txt", header = TRUE)
testing_data2 <- read.table("test_data.txt", header = TRUE)
```





```{r}
## newest submission using pca, cross validation & svm linear 
library(dplyr)
set.seed(2023)
training_data <- read.table("training_data.txt", header = TRUE)
testing_data <- read.table("test_data.txt", header = TRUE)
training_data$activity <- ifelse(training_data$activity %in% c("4", "5", "6"), 0, 1)
standardized.X <- as.data.frame(scale(training_data[,c(-1,-2)]))
testing_data_standardized <- as.data.frame(scale(testing_data[, -1]))

pca <- prcomp(standardized.X, center = TRUE, scale. = TRUE)
num_components <- 50
train_data_pca <- predict(pca, standardized.X)[,1:num_components]
test_data_pca <- predict(pca, testing_data_standardized)[,1:num_components]

cv <- trainControl(method = "cv", number = 10)
library(e1071)
svm_model <- svm(training_data$activity ~ ., data = train_data_pca, 
                 kernel = "linear", cost = 10, gamma = 0.1, trControl = cv)
my_predictions <- predict(svm_model, test_data_pca)

my_pred_binary <- ifelse(my_predictions > 0.5, 1, 0)
table(my_pred_binary)

write.table(my_pred_binary, "binary_0625.txt", 
           col.names = FALSE, row.names = FALSE, quote = FALSE)
```


```{r}
## newest multiclass using random forest with cv
trainControl <- trainControl(method = "cv", number = 10)
model2 <- randomForest(activity ~ ., data = training_data2, ntree = 100, mtry = 20, trControl = trainControl)

# Get predictions on the test set
pred2 <- predict(model2, newdata = testing_data2)
pred2 <- round(pred2)
pred2[pred2 >= 7] <- 7

table(pred2)
write.table(pred2, "multiclass_0625.txt", 
            col.names = FALSE, row.names = FALSE, quote = FALSE)
```

```{r}

nonstandardized.X <- as.data.frame(training_data2[,c(-1,-2)])

test <- 1:2267
train.X = nonstandardized.X[-test,]
test.X = nonstandardized.X[test,]
train.Y = training_data2$activity[-test]
test.Y = training_data2$activity[test]


trainControl <- trainControl(method = "cv", number = 10)
model2 <- randomForest(train.Y ~ ., data = train.X, ntree = 100, mtry = 20, trControl = trainControl)

# Get predictions on the test set
pred2 <- predict(model2, newdata = test.X)
pred2 <- round(pred2)
pred2[pred2 >= 7] <- 7
accuracy <- mean(pred2 == test.Y)
accuracy
```

```{r}
library(caret)
library(e1071)
# Preprocess data
preproc <- preProcess(train.X, method = c("center", "scale"))
train_data_scaled <- predict(preproc, train.X)
test_data_scaled <- predict(preproc, test.X)

# Train multiclass knn
knn_model <- train(x = train_data_scaled, y = train.Y, method = "knn",
                   trControl = trainControl(method = "cv", number = 5),
                   tuneLength = 5, preProc = c("center", "scale"))

# Make predictions on test data
predictions <- predict(knn_model, test_data_scaled)
predictions <- round(predictions)
predictions[predictions >= 7] <- 7
table(predictions)
# Calculate accuracy
accuracy <- mean(predictions == test.Y)
accuracy
```

```{r}
library(caret)
library(e1071)
# Preprocess data
preproc <- preProcess(train.X, method = c("center", "scale"))
train_data_scaled <- predict(preproc, train.X)
test_data_scaled <- predict(preproc, test.X)

# Train a Naive Bayes classifier
model <- naiveBayes(train.X, train.Y)

# Make predictions on the test data
predictions <- predict(model, test.X)
predictions <- as.numeric(predictions)
predictions[predictions >= 7] <- 7
table(predictions)
# Evaluate the accuracy of the classifier
accuracy <- mean(predictions == test.Y)

cat("Accuracy:", accuracy)


```
```{r}
library(caret)
library(e1071)

# Train a svm classifier
model <- svm(train.X, train.Y, cost = 10)

# Make predictions on the test data
predictions <- predict(model, test.X)
predictions <- round(predictions)
predictions[predictions >= 7] <- 7
table(predictions)
# Evaluate the accuracy of the classifier
accuracy <- mean(predictions == test.Y)
cat("Accuracy:", accuracy)
```


```{r}
Leaderboard_score <- data.frame(algorithm = c("XGBoost", "Logistic Regression","Neural Network", "SVM Linear", "SVM Radial", "SVM Linear with PCA and CV", "Random Forest with cv", "K-Nearest Neighbors", "naive bayes", "SVM Multiclass"),
                                problem = c("Binary","Binary","Binary","Binary","Binary", "Binary","Multiple","Multiple", "Multiple", "Multiple"),
                                accuracy = c(0.892453,0.773452, 0.923382,0.882523,0.878563, 0.995148, 0.89514,0.800618,0.621967, 0.700926)

)
Leaderboard_score
```






